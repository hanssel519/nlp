{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03db8fca",
   "metadata": {
    "id": "03db8fca"
   },
   "source": [
    "# Week 06: Dependency Parser and spacy\n",
    "The assignment this week is to identify the grammar pattern VERB-PREP-NOUN using two different methods. You will practice the various functionalities of spacy in the process. \n",
    "\n",
    "Data used in this assignment:  \n",
    "https://drive.google.com/file/d/1OIZPsDezgLaBjw3OX30YFyeFkzegtwP8/view?usp=sharing\n",
    "\n",
    "* sentences.s2orc.txt\n",
    "\n",
    "spacy tutorials:  \n",
    "https://www.machinelearningplus.com/spacy-tutorial-nlp/#phrasematcher  \n",
    "https://spacy.io/usage/linguistic-features#entity-linking\n",
    "\n",
    "## Requirements\n",
    "* pandas\n",
    "* spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da24123",
   "metadata": {
    "id": "8da24123"
   },
   "source": [
    "### Installation of spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f503a42",
   "metadata": {
    "id": "4f503a42",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (3.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (1.23.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (1.9.2)\n",
      "Requirement already satisfied: setuptools in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (44.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: en-core-web-sm==3.4.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl#egg=en_core_web_sm==3.4.1 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (3.4.1)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from en-core-web-sm==3.4.1) (3.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: setuptools in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (44.0.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.9.2)\n",
      "Requirement already satisfied: jinja2 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d6736c",
   "metadata": {
    "id": "55d6736c"
   },
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7719cbc-e82c-4d00-bc8f-7eaac0378b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/nlplab/yhc/env/nlp_hw/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa039179-37cf-4aa5-b037-8223ce421394",
   "metadata": {},
   "source": [
    "#### 動詞 介系詞 名詞\n",
    "raw base match: 直接抓連在一起的   \n",
    "tree: dependeancy--> 不用一定要連在一起 也抓得到    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d97fd4d",
   "metadata": {
    "id": "4d97fd4d",
    "outputId": "35b55654-7592-47d2-d975-31ea423e4617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence\n",
      "0  Meanwhile, an analysis of the literature shows...\n",
      "1  Meanwhile, this list can be supplemented with ...\n",
      "2  At the same time, in many cases, several instr...\n",
      "3  It is not possible to give a systematic assess...\n",
      "4  Correlation was calculated for the years, wher...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def loadData(path):\n",
    "    with open(path) as f:\n",
    "        sents = []\n",
    "        for line in f.readlines():\n",
    "            line = line.strip(\"\\n\").split(\"\\t\")\n",
    "            sents.append(line[1])\n",
    "    return pd.DataFrame({\"sentence\":sents})\n",
    "data = loadData(\"sentences.s2orc.txt\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2c07c81",
   "metadata": {
    "id": "f2c07c81",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef7d493",
   "metadata": {
    "id": "aef7d493"
   },
   "source": [
    "### Spacy example\n",
    "If you have any probelm, look up the documentation [here](https://spacy.io/usage/linguistic-features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "998b263c",
   "metadata": {
    "id": "998b263c"
   },
   "outputs": [],
   "source": [
    "example_text = \"\"\"The economic situation of the country is on edge , as the stock \n",
    "market crashed causing loss of millions. Citizens who had their main investment \n",
    "in the share-market are facing a great loss. Many companies might lay off \n",
    "thousands of people to reduce labor cost.\n",
    "He began immediately to rant about the gas price .\n",
    "\"\"\"\n",
    "\n",
    "# Remove newline character\n",
    "example_text = re.sub(\"\\n\", '', example_text)\n",
    "example_doc = nlp(example_text)\n",
    "#for token in example_doc:\n",
    "#    print(token.text,'--',token.is_stop,'---',token.is_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728770d5",
   "metadata": {
    "id": "728770d5"
   },
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Please print out the 2nd sentence in the example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03a85784",
   "metadata": {
    "id": "03a85784",
    "outputId": "785657b6-5167-46e4-a9ea-2c7973b89d3e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citizens who had their main investment in the share-market are facing a great loss.\n"
     ]
    }
   ],
   "source": [
    "sents = []\n",
    "for sent in example_doc.sents:\n",
    "    sents.append(sent)\n",
    "print(sents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f1e521",
   "metadata": {
    "id": "02f1e521"
   },
   "source": [
    "Let's start with some simple linguistic features we have been dealing with.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please print out the following token features of the first sentence in example_text:  \n",
    "text,  lemma,  POS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19548636-1ab7-47bd-8ba1-07b7529e876b",
   "metadata": {},
   "source": [
    "#### It is a sequence of tokens that contains not just the original text but all the results produced by the spaCy model after processing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1320b34",
   "metadata": {
    "id": "e1320b34",
    "outputId": "feeeb223-9ccb-406b-8b68-807b70dd23e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The the DET\n",
      "economic economic ADJ\n",
      "situation situation NOUN\n",
      "of of ADP\n",
      "the the DET\n",
      "country country NOUN\n",
      "is be AUX\n",
      "on on ADP\n",
      "edge edge NOUN\n",
      ", , PUNCT\n",
      "as as SCONJ\n",
      "the the DET\n",
      "stock stock NOUN\n",
      "market market NOUN\n",
      "crashed crash VERB\n",
      "causing cause VERB\n",
      "loss loss NOUN\n",
      "of of ADP\n",
      "millions million NOUN\n",
      ". . PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in sents[0]:\n",
    "    print(token.text, token.lemma_, token.pos_)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bba130",
   "metadata": {
    "id": "82bba130"
   },
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Data Process 1: Please run the s2orc data through spacy and store the result in data_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d4fb283",
   "metadata": {
    "id": "3d4fb283"
   },
   "outputs": [],
   "source": [
    "# 要跑一陣子：一次喂 再切\n",
    "# type(data) = dataframe\n",
    "data.head()\n",
    "data_doc = []\n",
    "for x in data['sentence']:\n",
    "    data_doc.append(nlp(x))\n",
    "#data_doc = nlp(data.loc[:, 'sentence'][:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05334eeb",
   "metadata": {
    "id": "05334eeb",
    "outputId": "a500c2be-8c53-4885-f69b-0ef1f1550508"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Meanwhile, an analysis of the literature shows that the development of indicators of financial stability has not yet been completed."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c07537a2-97c4-489e-9c20-272603d95182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Meanwhile, an analysis of the literature shows that the development of indicators of financial stability has not yet been completed.,\n",
       " Meanwhile, this list can be supplemented with instruments of monetary policy, which also have an impact on financial stability.,\n",
       " At the same time, in many cases, several instruments are used to reduce financial instability, which contributes to the achievement of various intermediate goals.,\n",
       " It is not possible to give a systematic assessment of financial stability and coordinate the use of monetary, macro-prudential and micro-prudential policies in order to reduce systemic risks.,\n",
       " Correlation was calculated for the years, where the information is available for both indicators.,\n",
       " Table 4 defines the criteria for market and institutional balance of financial stability, formed for the Russian economy.,\n",
       " The development of a risk map is necessary in order to determine the objects of regulation.,\n",
       " Blowing out a bubble has little effect on the asset itself.,\n",
       " In the state, the investment directions are tightly controlled, in private companies, there is a danger of their involvement in various risk schemes.,\n",
       " The decrease in IFS in early 2015 is due to a sudden increase in inflation at the end of 2014.]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_doc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa91c05",
   "metadata": {
    "id": "3aa91c05"
   },
   "source": [
    "### Named Entity Recognition\n",
    "Named Entity: a real-world object, such as a person, location, organization, product, etc., that can be denoted with a proper name.  \n",
    "\n",
    "The following is an example of named entity recognition using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d901938",
   "metadata": {
    "id": "5d901938",
    "outputId": "719095a5-1df0-43ad-938e-86e167d4e3dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada Lovelace PERSON\n",
      "New York GPE\n",
      "Thanksgiving DATE\n"
     ]
    }
   ],
   "source": [
    "# 找到專有名詞\n",
    "\n",
    "ner_doc = nlp(\"Ada Lovelace was born in New York at Thanksgiving.\")\n",
    "\n",
    "# Document level\n",
    "for e in ner_doc.ents:\n",
    "    print(e.text, e.label_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "545a45fd",
   "metadata": {
    "id": "545a45fd",
    "outputId": "d36fdf2a-f88b-4abb-9625-f5bc43c2f731"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ada Lovelace\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " was born in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    New York\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " at \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Thanksgiving\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(ner_doc,style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a00997d",
   "metadata": {
    "id": "6a00997d"
   },
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Data Process 2: Please replace all named entities in data_doc with their labels.  \n",
    "For example,  \n",
    "\"Ada Lovelace was born in New York at Thanksgiving.\" should be adjusted to  \n",
    "\"PERSON was born in GPE at DATE.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f1dbd-045a-4799-a04d-573f25e73fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_doc1 = []\n",
    "\n",
    "for doc in data_doc:\n",
    "    s = doc.text\n",
    "    for ent in doc.ents:\n",
    "        s = s.replace(ent.text, ent.label_)\n",
    "    data_doc1.append(nlp(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7bf0d-bf57-41be-a321-c9353b567cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_doc1[33] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57840735-57e9-47a2-bad4-2ca03a98f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_doc = data_doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4933ded-f3d6-4a59-b108-9faf0f33b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_doc[:10]\n",
    "#print(len(data_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa97686",
   "metadata": {
    "id": "efa97686"
   },
   "source": [
    "### Dependency Parser\n",
    "\n",
    "If you have probelms concerning the dependency parser tags, look up the documentation [here](https://universaldependencies.org/en/dep/index.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca13c64",
   "metadata": {
    "id": "4ca13c64",
    "outputId": "ced50ef0-728e-41d0-bc44-53d21db06db4"
   },
   "outputs": [],
   "source": [
    "# Example of Dependency Parser\n",
    "# dep wo _ 是id\n",
    "print(sents[2])\n",
    "for token in sents[2]:\n",
    "    #print(token.text, token.dep_)\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc85cf22",
   "metadata": {
    "id": "cc85cf22",
    "outputId": "1738d42a-284a-490e-ed0c-f40bdccd7af0"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(sents[2], style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6e1a82",
   "metadata": {
    "id": "cb6e1a82"
   },
   "source": [
    "To traverse a dependency tree, use the following properties of token object.  \n",
    "token.children, token.lefts, token.rights  \n",
    "\n",
    "If you have any probelms, please check [here](https://spacy.io/api/token#children)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75901966",
   "metadata": {
    "id": "75901966"
   },
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Please identify a VERB-PREP-NOUN grammar structure in sent[2] by traversing the dependency tree.  \n",
    "Expected output:  \n",
    "(lay, off, thousands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab2634-d116-4679-b29f-7cebdd6e8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_right(token):\n",
    "    return_set = []\n",
    "    # type1: v -> p -> n\n",
    "    if token.pos_ == 'ADP':\n",
    "        rights = [t for t in token.rights]\n",
    "        #check 有無 n\n",
    "        for child in rights:\n",
    "            if child.pos_ == 'NOUN':\n",
    "                return_set.append(tuple((token, child)))\n",
    "    # type2: v -> n -> p\n",
    "    if token.pos_ == 'NOUN':\n",
    "        lefts = [t for t in token.lefts]\n",
    "        #check 左側有無 p\n",
    "        for child in lefts:\n",
    "            if child.pos_ == 'ADP':\n",
    "                return_set.append(tuple((token, child)))\n",
    "    \n",
    "    return return_set #list of tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cec92b7",
   "metadata": {
    "id": "9cec92b7"
   },
   "outputs": [],
   "source": [
    "col_list = [] \n",
    "for token in sents[2]:    \n",
    "    print(token.text, token.pos_, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])\n",
    "    if token.pos_ == 'VERB':\n",
    "        total = []\n",
    "        rights = [t for t in token.rights]\n",
    "        #print(f'\\t {token.text}\\trights:  {rights}\\n')\n",
    "        \n",
    "        for t in rights:\n",
    "            #print(t.text)\n",
    "            # type( verb_right(t) ) = [(), () ...] --> list_of_tuples\n",
    "            total.extend(list_of_tuples for list_of_tuples in verb_right(t))\n",
    "            #print(f'\\t total:  {total}\\n')\n",
    "            # TODO = total might have duplicate / \n",
    "        count = 0\n",
    "        for r_child in rights:\n",
    "            count += 1\n",
    "            if r_child.pos_ == 'ADP':\n",
    "                for i in range(count, len(rights)):\n",
    "                    #print(f'i:   {i}')\n",
    "                    if rights[i].pos_ == 'NOUN':\n",
    "                        total.append(tuple((r_child, rights[i])))\n",
    "                        #print(type(total[-1]))\n",
    "                            #break # 只找一個\n",
    "        #print(f'\\n\\n final total\\t\\t{total}')\n",
    "        for x in total:\n",
    "            #print(type(x))\n",
    "            #print(x)\n",
    "            col_list.append(tuple((token, x[0], x[1])))\n",
    "            \n",
    "print('\\n\\n')            \n",
    "print(col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4e46d2",
   "metadata": {
    "id": "1f4e46d2"
   },
   "source": [
    "<font color=\"red\">**[ TODO ]**</font>  Please identify all VERB-PREP-NOUN grammar structure in data_doc by traversing the dependency trees and save the results in a list of tuples dep_gp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d7c17",
   "metadata": {
    "id": "c25d7c17"
   },
   "outputs": [],
   "source": [
    "dep_gp = []\n",
    "\n",
    "for sentence in data_doc:\n",
    "    col_list = [] \n",
    "    for token in sentence:\n",
    "        if token.pos_ == 'VERB':\n",
    "            total = []\n",
    "            rights = [t for t in token.rights]\n",
    "\n",
    "            for t in rights:\n",
    "                # type( verb_right(t) ) = [(), () ...] --> list_of_tuples\n",
    "                total.extend(list_of_tuples for list_of_tuples in verb_right(t))\n",
    "                #print(f'\\t total:  {total}\\n')\n",
    "                # TODO = total might have duplicate / \n",
    "            count = 0\n",
    "            for r_child in rights:\n",
    "                count += 1\n",
    "                if r_child.pos_ == 'ADP':\n",
    "                    for i in range(count, len(rights)):\n",
    "                        #print(f'i:   {i}')\n",
    "                        if rights[i].pos_ == 'NOUN':\n",
    "                            total.append(tuple((r_child, rights[i])))\n",
    "                                #break # 只找一個\n",
    "            for x in total:\n",
    "                col_list.append(tuple((token, x[0], x[1])))\n",
    "           \n",
    "    #print(col_list)\n",
    "    dep_gp.extend(col_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b5c69",
   "metadata": {
    "id": "3e4b5c69"
   },
   "source": [
    "<font color=\"red\">**[ TODO ]**</font>  Please print out all VERB-PREP-NOUN grammar patterns in dep_gp with the verb \"charge\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48da0488",
   "metadata": {
    "id": "48da0488"
   },
   "outputs": [],
   "source": [
    "# print verb有 provide 的 !!!\n",
    "print(dep_gp[:20])\n",
    "print(len(dep_gp))\n",
    "print('*' *50)\n",
    "provide_list = []\n",
    "# provide_list = [x if x[0].text == 'provide' x for x in dep_gp]\n",
    "for x in dep_gp:\n",
    "    if x[0].lemma_ == 'provide':\n",
    "        provide_list.append(x)\n",
    "print(len(provide_list))\n",
    "print(provide_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee87eef",
   "metadata": {
    "id": "5ee87eef"
   },
   "source": [
    "### Rule Based Methods \n",
    "We can also custom build rules for spacy to match patterns.  \n",
    "[Documentation](https://spacy.io/api/matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74664296",
   "metadata": {
    "id": "74664296"
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9b3d4",
   "metadata": {
    "id": "abc9b3d4"
   },
   "outputs": [],
   "source": [
    "# Example text\n",
    "text = \"\"\"I visited Manali last time . Around same budget trips ? I was visiting Ladakh this summer . I have planned visiting New York and other abroad places for next year. Have you ever visited Kodaikanal? \"\"\"\n",
    "text = re.sub('\\n', '', text)\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d053b2f",
   "metadata": {
    "id": "9d053b2f",
    "outputId": "0d9e93f9-4603-43e8-fc99-645a9fe1dad5"
   },
   "outputs": [],
   "source": [
    "# Initialize the matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Write a pattern that matches a form of \"visit\" + place\n",
    "my_pattern = [{\"LEMMA\": \"visit\"}, {\"POS\": \"PROPN\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"Visting_places\", [my_pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Counting the no of matches\n",
    "print(\" matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b258c",
   "metadata": {
    "id": "065b258c"
   },
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Please identify all VERB-PREP-NOUN grammar structure in data_doc by applying a matcher rule and store the results in a list of tuples rule_gp. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f683e-f709-45d5-82d1-adf6dddcd5bf",
   "metadata": {},
   "source": [
    "a+b+c 中間如果有其他字 --> 應該抓不到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7076373d",
   "metadata": {
    "id": "7076373d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#用lemma \n",
    "\n",
    "rule_gp = []\n",
    "\n",
    "\n",
    "# Initialize the matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Write a pattern that matches a form of \"visit\" + place\n",
    "my_pattern = [ {\"POS\": \"VERB\"}, {\"POS\": \"ADP\"},  {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"Visting_places\", [my_pattern])\n",
    "\n",
    "for data_nlp in data_doc:\n",
    "    matches = matcher(data_nlp)\n",
    "    for match_id, start, end in matches:\n",
    "        #print(\"Match found:\", data_nlp[start:end].text)\n",
    "        rule_gp.append(tuple(data_nlp[start:end].text.split()))\n",
    "# Counting the no of matches\n",
    "print(len(rule_gp))\n",
    "#print(rule_gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447e55b-0102-4490-9000-3a80ec093a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_gp[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a03652",
   "metadata": {
    "id": "a4a03652"
   },
   "source": [
    "<font color=\"red\">**[ TODO ]**</font>  Please print out all VERB-PREP-NOUN grammar patterns in rule_gp with the verb \"charge\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918eeda9",
   "metadata": {
    "id": "918eeda9"
   },
   "outputs": [],
   "source": [
    "for x in rule_gp:\n",
    "    if x[0] == 'provided':\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efbb4d",
   "metadata": {
    "id": "69efbb4d"
   },
   "source": [
    "## TA's Notes\n",
    "\n",
    "If you complete the Assignment, please use [this link](https://docs.google.com/spreadsheets/d/1OKbXhcv6E3FEQDPnbHEHEeHvpxv01jxugMP7WwnKqKw/edit#gid=258852025) to reserve demo time.  \n",
    "The score is only given after TAs review your implementation, so <u>**make sure you make a appointment with a TA before you miss the deadline**</u> .  <br>After demo, please upload your assignment to elearn. You just need to hand in this ipynb file and rename it as XXXXXXXXX(Your student ID).ipynb.\n",
    "<br>Note that **late submission will not be allowed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce87b3-aa89-43eb-b4ac-0a217c43c000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

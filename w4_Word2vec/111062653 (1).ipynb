{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tdj1XLuceOk-"
   },
   "source": [
    "# Week 03: Word Representation\n",
    "The assignment this week is to distinguish between good and bad phrases of the word \"**earn**\" (e.g., earn money). You will practice using word2vector,  one of the methods learned today, in the process. \n",
    "\n",
    "Data used in this assignment:  \n",
    "https://drive.google.com/drive/folders/1qTIrefo4EFbsVF3LXhKbiahbIrvCLUBJ?usp=sharing\n",
    "\n",
    "* train.tsv: Some phrases with labels to train and validate the classification model. There are only two types of label: 1 means *good*; 0 means *bad*.\n",
    "* test.tsv: Same format as train.tsv. It's used to test your model.\n",
    "* GoogleNews-vectors-negative300.bin.gz: a pre-trained word2vector model trained by Google ([source](https://code.google.com/archive/p/word2vec/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GzvI76xeOlH"
   },
   "source": [
    "## Requirements\n",
    "* pandas\n",
    "* tensorflow\n",
    "* sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jk5Xag5ueOlI"
   },
   "source": [
    "## Read Data\n",
    "We use dataframe to store data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UYsjz2eCeOlI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         phrase  class\n",
      "0      earn a strong reputation      1\n",
      "1  Marty will surely earn every      0\n",
      "2             to earn between $      0\n",
      "3          to earn some college      0\n",
      "4        that earn rave reviews      0\n",
      "                   phrase  class\n",
      "0  degree earn 62 percent      0\n",
      "1     earn maybe 30 or 50      0\n",
      "2  earn the kind of money      1\n",
      "3      earn his 14th save      1\n",
      "4   earn a smaller amount      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def loadData(path):\n",
    "    ngram = []\n",
    "    _class = []\n",
    "    with open(path) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip(\"\\n\").split(\"\\t\")\n",
    "            ngram.append(line[0])\n",
    "            _class.append(int(line[1]))\n",
    "    return pd.DataFrame({\"phrase\":ngram,\"class\":_class})\n",
    "train = loadData(\"train.tsv\")\n",
    "print(train.head())\n",
    "test = loadData(\"test.tsv\")\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sl8pGQx2eOlL"
   },
   "source": [
    "## load word2vec model\n",
    "<font color=\"red\">**[ TODO ]**</font> Please load [GoogleNews-vectors-negative300.bin.gz](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g) model and check the embedding of the word `language`.\n",
    "\n",
    "* package `gensim` is a good choice (Look up the documentation [here](https://radimrehurek.com/gensim/models/word2vec.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DQjCDqZyeOlO"
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "#model = Word2Vec.load('public_data/GoogleNews-vectors-negative300.bin.gz')\n",
    "w2v_model = models.KeyedVectors.load_word2vec_format(\n",
    "    'public_data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "#### print \"language\" embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.30712891e-02  1.68457031e-02  1.54296875e-01  1.27929688e-01\n",
      " -2.67578125e-01  3.51562500e-02  1.19140625e-01  2.48046875e-01\n",
      "  1.93359375e-01 -7.95898438e-02  1.46484375e-01 -1.43554688e-01\n",
      " -3.04687500e-01  3.46679688e-02 -1.85546875e-02  1.06933594e-01\n",
      " -1.52343750e-01  2.89062500e-01  2.35595703e-02 -3.80859375e-01\n",
      "  1.09863281e-01  4.41406250e-01  3.75976562e-02 -1.22680664e-02\n",
      "  1.62353516e-02 -2.24609375e-01  7.61718750e-02 -3.12500000e-02\n",
      " -2.16064453e-02  1.49414062e-01 -4.02832031e-02 -4.46777344e-02\n",
      " -1.72851562e-01  3.32031250e-02  1.50390625e-01 -5.05371094e-02\n",
      "  2.72216797e-02  3.00781250e-01 -1.33789062e-01 -7.56835938e-02\n",
      "  1.93359375e-01 -1.98242188e-01 -1.27563477e-02  4.19921875e-01\n",
      " -2.19726562e-01  1.44531250e-01 -3.93066406e-02  1.94335938e-01\n",
      " -3.12500000e-01  1.84570312e-01  1.48773193e-04 -1.67968750e-01\n",
      " -7.37304688e-02 -3.12500000e-02  1.57226562e-01  3.30078125e-01\n",
      " -1.42578125e-01 -3.16406250e-01 -7.32421875e-02 -5.76171875e-02\n",
      "  1.02050781e-01 -1.08886719e-01  1.24023438e-01 -2.50244141e-02\n",
      " -2.49023438e-01  1.25976562e-01 -1.79687500e-01  3.32031250e-01\n",
      "  7.14111328e-03  2.51953125e-01  4.34570312e-02 -4.34570312e-02\n",
      " -3.90625000e-01  1.76757812e-01 -1.13525391e-02 -1.97753906e-02\n",
      "  2.79296875e-01  2.36328125e-01  1.19140625e-01  5.59082031e-02\n",
      "  1.73828125e-01 -1.10839844e-01 -4.95605469e-02  2.13867188e-01\n",
      "  6.17675781e-02  1.38671875e-01 -4.45556641e-03  2.55859375e-01\n",
      "  1.80664062e-01  5.88378906e-02 -6.59179688e-02 -2.08007812e-01\n",
      " -1.19140625e-01 -1.57226562e-01  5.02929688e-02 -6.29882812e-02\n",
      "  5.00488281e-02 -7.27539062e-02  1.74560547e-02 -3.56445312e-02\n",
      " -1.93359375e-01  3.93066406e-02 -3.36914062e-02 -1.07421875e-01\n",
      "  5.78613281e-02 -8.20312500e-02  1.74560547e-02 -1.65039062e-01\n",
      "  1.46484375e-01 -3.08837891e-02 -3.86718750e-01  2.49023438e-01\n",
      "  8.74023438e-02 -2.15820312e-01 -4.10156250e-02  1.60156250e-01\n",
      "  1.85546875e-01 -2.27050781e-02 -3.73535156e-02  7.86132812e-02\n",
      " -1.46484375e-01  6.78710938e-02  1.26953125e-01  3.30078125e-01\n",
      "  1.11328125e-01  9.27734375e-02 -3.45703125e-01 -1.41601562e-01\n",
      " -5.29785156e-02 -1.50390625e-01 -7.81250000e-02 -1.27929688e-01\n",
      " -4.02343750e-01 -1.41601562e-01  8.44726562e-02  1.08398438e-01\n",
      " -4.44335938e-02  3.73535156e-02  5.61523438e-02 -1.91406250e-01\n",
      "  1.54296875e-01 -5.12695312e-02 -6.49414062e-02 -8.30078125e-02\n",
      "  7.17773438e-02 -1.33789062e-01  1.05468750e-01  3.33984375e-01\n",
      " -1.08398438e-01  1.91650391e-02  2.14843750e-01  2.15820312e-01\n",
      " -1.05468750e-01 -1.44531250e-01  4.32128906e-02 -2.71484375e-01\n",
      " -3.78906250e-01  1.09863281e-01 -8.15429688e-02 -6.12792969e-02\n",
      " -1.33789062e-01  9.71679688e-02 -1.04370117e-02 -1.21093750e-01\n",
      " -2.44140625e-01  1.02050781e-01  1.10839844e-01 -1.00585938e-01\n",
      "  1.71875000e-01 -3.61328125e-02 -4.39453125e-02  2.83203125e-01\n",
      " -8.93554688e-02 -1.70898438e-01  2.46093750e-01  1.16699219e-01\n",
      "  8.39843750e-02 -1.32812500e-01 -1.61132812e-01 -1.39648438e-01\n",
      " -8.59375000e-02 -1.37695312e-01 -9.32617188e-02 -1.33789062e-01\n",
      "  1.65039062e-01  4.93164062e-02 -1.21093750e-01 -2.11914062e-01\n",
      "  1.61132812e-01 -1.07421875e-01 -3.97949219e-02 -3.51562500e-01\n",
      " -5.02929688e-02  1.46484375e-01 -4.68750000e-02  4.17480469e-02\n",
      " -1.27929688e-01 -9.76562500e-02 -2.46093750e-01  6.78710938e-02\n",
      " -2.30468750e-01  1.80664062e-02  3.54003906e-02  7.32421875e-02\n",
      " -2.23632812e-01 -1.25976562e-01  2.12890625e-01 -3.93066406e-02\n",
      " -2.41699219e-02 -9.61914062e-02  7.51953125e-02 -1.46484375e-01\n",
      " -1.49414062e-01 -8.83789062e-02 -4.88281250e-02  2.32421875e-01\n",
      "  3.30078125e-01  1.59179688e-01 -2.35351562e-01 -1.25976562e-01\n",
      "  2.68554688e-02 -5.29785156e-02 -6.59179688e-02 -2.17773438e-01\n",
      " -6.37817383e-03 -2.53906250e-01  2.28515625e-01  4.93164062e-02\n",
      "  3.54003906e-02  1.66992188e-01 -7.27539062e-02 -2.53906250e-01\n",
      " -1.34765625e-01  3.69140625e-01  1.83593750e-01 -1.64062500e-01\n",
      "  2.26562500e-01 -8.88671875e-02  3.69140625e-01  5.54199219e-02\n",
      " -3.63769531e-02 -1.48437500e-01  9.13085938e-02  2.47955322e-04\n",
      "  2.67578125e-01 -1.63085938e-01  1.19628906e-01  2.77343750e-01\n",
      " -1.49414062e-01  1.33789062e-01 -8.25195312e-02 -1.74804688e-01\n",
      " -1.77734375e-01  2.06054688e-01  5.07812500e-02 -2.08007812e-01\n",
      " -1.74804688e-01  9.66796875e-02  6.98242188e-02 -5.79833984e-04\n",
      "  9.22851562e-02  7.95898438e-02  1.41601562e-01  8.72802734e-03\n",
      " -8.05664062e-02  4.80957031e-02  2.49023438e-01 -1.64062500e-01\n",
      " -4.66308594e-02 -2.81250000e-01 -1.66015625e-01 -2.22656250e-01\n",
      " -2.32421875e-01  1.32812500e-01  4.15039062e-02  1.15234375e-01\n",
      " -7.66601562e-02 -1.10839844e-01 -1.97265625e-01  3.06396484e-02\n",
      " -1.03515625e-01  2.49023438e-02 -2.52685547e-02  3.39355469e-02\n",
      "  4.29687500e-02 -1.44531250e-01  2.12402344e-02  2.28271484e-02\n",
      " -1.88476562e-01  3.22265625e-01 -1.13281250e-01 -7.61718750e-02\n",
      "  2.94921875e-01 -1.33789062e-01 -1.80664062e-02 -6.25610352e-03\n",
      " -1.62353516e-02  5.98144531e-02  1.21582031e-01  4.17480469e-02]\n"
     ]
    }
   ],
   "source": [
    "vector = w2v_model['language']\n",
    "print(vector)\n",
    "#print(len(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fap51QcAeOlO",
    "tags": []
   },
   "source": [
    "<font color=\"green\">Expected output: </font>\n",
    "\n",
    ">  <font face='monospace' size=3>\\[&nbsp;2.30712891e-02&nbsp;&nbsp;1.68457031e-02&nbsp;&nbsp;1.54296875e-01&nbsp; 1.27929688e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.67578125e-01&nbsp;&nbsp;3.51562500e-02&nbsp;&nbsp;1.19140625e-01&nbsp; 2.48046875e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.93359375e-01&nbsp;-7.95898438e-02&nbsp;&nbsp;1.46484375e-01&nbsp;-1.43554688e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-3.04687500e-01&nbsp;&nbsp;3.46679688e-02&nbsp;-1.85546875e-02&nbsp; 1.06933594e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.52343750e-01&nbsp;&nbsp;2.89062500e-01&nbsp;&nbsp;2.35595703e-02&nbsp;-3.80859375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.09863281e-01&nbsp;&nbsp;4.41406250e-01&nbsp;&nbsp;3.75976562e-02&nbsp;-1.22680664e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.62353516e-02&nbsp;-2.24609375e-01&nbsp;&nbsp;7.61718750e-02&nbsp;-3.12500000e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.16064453e-02&nbsp;&nbsp;1.49414062e-01&nbsp;-4.02832031e-02&nbsp;-4.46777344e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.72851562e-01&nbsp;&nbsp;3.32031250e-02&nbsp;&nbsp;1.50390625e-01&nbsp;-5.05371094e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.72216797e-02&nbsp;&nbsp;3.00781250e-01&nbsp;-1.33789062e-01&nbsp;-7.56835938e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.93359375e-01&nbsp;-1.98242188e-01&nbsp;-1.27563477e-02&nbsp; 4.19921875e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.19726562e-01&nbsp;&nbsp;1.44531250e-01&nbsp;-3.93066406e-02&nbsp; 1.94335938e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-3.12500000e-01&nbsp;&nbsp;1.84570312e-01&nbsp;&nbsp;1.48773193e-04&nbsp;-1.67968750e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-7.37304688e-02&nbsp;-3.12500000e-02&nbsp;&nbsp;1.57226562e-01&nbsp; 3.30078125e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.42578125e-01&nbsp;-3.16406250e-01&nbsp;-7.32421875e-02&nbsp;-5.76171875e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.02050781e-01&nbsp;-1.08886719e-01&nbsp;&nbsp;1.24023438e-01&nbsp;-2.50244141e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.49023438e-01&nbsp;&nbsp;1.25976562e-01&nbsp;-1.79687500e-01&nbsp; 3.32031250e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;7.14111328e-03&nbsp;&nbsp;2.51953125e-01&nbsp;&nbsp;4.34570312e-02&nbsp;-4.34570312e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-3.90625000e-01&nbsp;&nbsp;1.76757812e-01&nbsp;-1.13525391e-02&nbsp;-1.97753906e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.79296875e-01&nbsp;&nbsp;2.36328125e-01&nbsp;&nbsp;1.19140625e-01&nbsp; 5.59082031e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.73828125e-01&nbsp;-1.10839844e-01&nbsp;-4.95605469e-02&nbsp; 2.13867188e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;6.17675781e-02&nbsp;&nbsp;1.38671875e-01&nbsp;-4.45556641e-03&nbsp; 2.55859375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.80664062e-01&nbsp;&nbsp;5.88378906e-02&nbsp;-6.59179688e-02&nbsp;-2.08007812e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.19140625e-01&nbsp;-1.57226562e-01&nbsp;&nbsp;5.02929688e-02&nbsp;-6.29882812e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;5.00488281e-02&nbsp;-7.27539062e-02&nbsp;&nbsp;1.74560547e-02&nbsp;-3.56445312e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.93359375e-01&nbsp;&nbsp;3.93066406e-02&nbsp;-3.36914062e-02&nbsp;-1.07421875e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;5.78613281e-02&nbsp;-8.20312500e-02&nbsp;&nbsp;1.74560547e-02&nbsp;-1.65039062e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.46484375e-01&nbsp;-3.08837891e-02&nbsp;-3.86718750e-01&nbsp; 2.49023438e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;8.74023438e-02&nbsp;-2.15820312e-01&nbsp;-4.10156250e-02&nbsp; 1.60156250e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.85546875e-01&nbsp;-2.27050781e-02&nbsp;-3.73535156e-02&nbsp; 7.86132812e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.46484375e-01&nbsp;&nbsp;6.78710938e-02&nbsp;&nbsp;1.26953125e-01&nbsp; 3.30078125e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.11328125e-01&nbsp;&nbsp;9.27734375e-02&nbsp;-3.45703125e-01&nbsp;-1.41601562e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-5.29785156e-02&nbsp;-1.50390625e-01&nbsp;-7.81250000e-02&nbsp;-1.27929688e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-4.02343750e-01&nbsp;-1.41601562e-01&nbsp;&nbsp;8.44726562e-02&nbsp; 1.08398438e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-4.44335938e-02&nbsp;&nbsp;3.73535156e-02&nbsp;&nbsp;5.61523438e-02&nbsp;-1.91406250e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.54296875e-01&nbsp;-5.12695312e-02&nbsp;-6.49414062e-02&nbsp;-8.30078125e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;7.17773438e-02&nbsp;-1.33789062e-01&nbsp;&nbsp;1.05468750e-01&nbsp; 3.33984375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.08398438e-01&nbsp;&nbsp;1.91650391e-02&nbsp;&nbsp;2.14843750e-01&nbsp; 2.15820312e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.05468750e-01&nbsp;-1.44531250e-01&nbsp;&nbsp;4.32128906e-02&nbsp;-2.71484375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-3.78906250e-01&nbsp;&nbsp;1.09863281e-01&nbsp;-8.15429688e-02&nbsp;-6.12792969e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.33789062e-01&nbsp;&nbsp;9.71679688e-02&nbsp;-1.04370117e-02&nbsp;-1.21093750e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.44140625e-01&nbsp;&nbsp;1.02050781e-01&nbsp;&nbsp;1.10839844e-01&nbsp;-1.00585938e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.71875000e-01&nbsp;-3.61328125e-02&nbsp;-4.39453125e-02&nbsp; 2.83203125e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-8.93554688e-02&nbsp;-1.70898438e-01&nbsp;&nbsp;2.46093750e-01&nbsp; 1.16699219e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;8.39843750e-02&nbsp;-1.32812500e-01&nbsp;-1.61132812e-01&nbsp;-1.39648438e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-8.59375000e-02&nbsp;-1.37695312e-01&nbsp;-9.32617188e-02&nbsp;-1.33789062e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.65039062e-01&nbsp;&nbsp;4.93164062e-02&nbsp;-1.21093750e-01&nbsp;-2.11914062e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.61132812e-01&nbsp;-1.07421875e-01&nbsp;-3.97949219e-02&nbsp;-3.51562500e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-5.02929688e-02&nbsp;&nbsp;1.46484375e-01&nbsp;-4.68750000e-02&nbsp; 4.17480469e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.27929688e-01&nbsp;-9.76562500e-02&nbsp;-2.46093750e-01&nbsp; 6.78710938e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.30468750e-01&nbsp;&nbsp;1.80664062e-02&nbsp;&nbsp;3.54003906e-02&nbsp; 7.32421875e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.23632812e-01&nbsp;-1.25976562e-01&nbsp;&nbsp;2.12890625e-01&nbsp;-3.93066406e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.41699219e-02&nbsp;-9.61914062e-02&nbsp;&nbsp;7.51953125e-02&nbsp;-1.46484375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.49414062e-01&nbsp;-8.83789062e-02&nbsp;-4.88281250e-02&nbsp; 2.32421875e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;3.30078125e-01&nbsp;&nbsp;1.59179688e-01&nbsp;-2.35351562e-01&nbsp;-1.25976562e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.68554688e-02&nbsp;-5.29785156e-02&nbsp;-6.59179688e-02&nbsp;-2.17773438e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-6.37817383e-03&nbsp;-2.53906250e-01&nbsp;&nbsp;2.28515625e-01&nbsp; 4.93164062e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;3.54003906e-02&nbsp;&nbsp;1.66992188e-01&nbsp;-7.27539062e-02&nbsp;-2.53906250e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.34765625e-01&nbsp;&nbsp;3.69140625e-01&nbsp;&nbsp;1.83593750e-01&nbsp;-1.64062500e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.26562500e-01&nbsp;-8.88671875e-02&nbsp;&nbsp;3.69140625e-01&nbsp; 5.54199219e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-3.63769531e-02&nbsp;-1.48437500e-01&nbsp;&nbsp;9.13085938e-02&nbsp; 2.47955322e-04<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.67578125e-01&nbsp;-1.63085938e-01&nbsp;&nbsp;1.19628906e-01&nbsp; 2.77343750e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.49414062e-01&nbsp;&nbsp;1.33789062e-01&nbsp;-8.25195312e-02&nbsp;-1.74804688e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.77734375e-01&nbsp;&nbsp;2.06054688e-01&nbsp;&nbsp;5.07812500e-02&nbsp;-2.08007812e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.74804688e-01&nbsp;&nbsp;9.66796875e-02&nbsp;&nbsp;6.98242188e-02&nbsp;-5.79833984e-04<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;9.22851562e-02&nbsp;&nbsp;7.95898438e-02&nbsp;&nbsp;1.41601562e-01&nbsp; 8.72802734e-03<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-8.05664062e-02&nbsp;&nbsp;4.80957031e-02&nbsp;&nbsp;2.49023438e-01&nbsp;-1.64062500e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-4.66308594e-02&nbsp;-2.81250000e-01&nbsp;-1.66015625e-01&nbsp;-2.22656250e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.32421875e-01&nbsp;&nbsp;1.32812500e-01&nbsp;&nbsp;4.15039062e-02&nbsp; 1.15234375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-7.66601562e-02&nbsp;-1.10839844e-01&nbsp;-1.97265625e-01&nbsp; 3.06396484e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.03515625e-01&nbsp;&nbsp;2.49023438e-02&nbsp;-2.52685547e-02&nbsp; 3.39355469e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;4.29687500e-02&nbsp;-1.44531250e-01&nbsp;&nbsp;2.12402344e-02&nbsp; 2.28271484e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.88476562e-01&nbsp;&nbsp;3.22265625e-01&nbsp;-1.13281250e-01&nbsp;-7.61718750e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.94921875e-01&nbsp;-1.33789062e-01&nbsp;-1.80664062e-02&nbsp;-6.25610352e-03<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.62353516e-02&nbsp;&nbsp;5.98144531e-02&nbsp;&nbsp;1.21582031e-01&nbsp; 4.17480469e-02\\] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RL4Gqhyw56oX"
   },
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> You can also find the top-N most similar words. Try it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zq-Jwhxe5jDy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('elementary', 0.7868632078170776), ('schools', 0.7411909103393555), ('shool', 0.6692329049110413), ('elementary_schools', 0.6597154140472412), ('kindergarten', 0.6529810428619385)]\n"
     ]
    }
   ],
   "source": [
    "#### print top 5 most similar words to \"school\"\n",
    "vector = w2v_model['school']  # get numpy vector of a word\n",
    "sims = w2v_model.most_similar('school', topn=5)  # get other similar words\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUUOFU4J4Anl"
   },
   "source": [
    "<font color=\"green\">Expected output: </font>\n",
    ">  <font face='monospace' size=3>\n",
    "[('elementary', 0.7868632078170776),<br>\n",
    "&nbsp;('schools', 0.7411909103393555),<br>\n",
    "&nbsp;('shool', 0.6692329049110413),<br>\n",
    "&nbsp;('elementary_schools', 0.6597153544425964),<br>\n",
    "&nbsp;('kindergarten', 0.6529811024665833)]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIk5hWfGeOlR"
   },
   "source": [
    "## Preprocessing\n",
    "Preprocess the two tsv files here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUKN7pEKeOlS"
   },
   "source": [
    "#### adjust the ratio of the two classes of training data\n",
    "In the training data, the ratio of good phrases to bad phrases is about one to thirty. That will make training classification unsatisfactory, so we need to adjust the ratio. Reducing bad phrases and adding good phrases are both common way.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please adjust the ratio of good phrases to bad phrases however you think is best and output the number of the two classes for demo.\n",
    "\n",
    "You need to explain why you chose this ratio and how you did it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "good & bad 比例太懸殊 會導致全部自動判斷為good 或bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "gpleKkC9eOlS",
    "outputId": "c8fe9973-707a-4b08-be22-c9532f76fe3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(36630, 2)\n",
      "Index(['phrase', 'class'], dtype='object')\n",
      "6105\n",
      "30525\n",
      "                                phrase  class\n",
      "108624         what come round to earn      1\n",
      "107679            earn anywhere from 1      0\n",
      "62379       advisors earn their living      0\n",
      "191580  earn an average monthly salary      1\n",
      "99647     earn their living with their      0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import random\n",
    "print(type(train))\n",
    "print(train.shape)\n",
    "print(train.columns)\n",
    "\n",
    "train_1 = train.loc[train['class'] == 1]\n",
    "train_0 = train.loc[train['class'] == 0]\n",
    "\n",
    "print(len(train_1))\n",
    "print(len(train_0))\n",
    "train_0 = train_0.sample(n=len(train_1)*5, random_state = 20)\n",
    "\n",
    "train = [train_1, train_0]\n",
    "\n",
    "train = pd.concat(train, axis=0)\n",
    "train = shuffle(train)\n",
    "print(train.head())\n",
    "\n",
    "\n",
    "\n",
    "#### print the number of training data of two classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V40xvY1eOlT"
   },
   "source": [
    "#### number words\n",
    "Give each word a unique number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "N2g3NLJ9eOlT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5798\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(pd.concat([train,test],ignore_index=True)['phrase'])\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras_preprocessing.text.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sj0V1G_AeOlT"
   },
   "source": [
    "#### convert phrases into numbers\n",
    "Your model can't understand words, so we have to do this transform first. \n",
    "\n",
    "The number should be the same as the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FdwSF-1JeOlU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45, 1025, 580, 2, 1], [1, 609, 23, 52], [2541, 1, 18, 29], [1, 14, 81, 244, 113], [1, 18, 29, 66, 18], [1, 871, 15], [93, 235, 2, 1], [1, 405, 56, 954, 35], [2, 1, 3, 529], [26, 107, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "# 把每個phrase轉成number\n",
    "train_encoded_phrase = tok.texts_to_sequences(train['phrase'])\n",
    "print(train_encoded_phrase[:10])\n",
    "test_encoded_phrase = tok.texts_to_sequences(test['phrase'])\n",
    "\n",
    "# [182, 1, 3, 153] number in list is id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 136, 172, 182, 123]\n",
      "[1, 342, 77, 186, 30, 342, 96, 186]\n"
     ]
    }
   ],
   "source": [
    "print(max(test_encoded_phrase, key=len)) # =6?\n",
    "print(max(train_encoded_phrase, key=len)) # =8?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hhTKdm7eOlV"
   },
   "source": [
    "#### <font color=\"red\">**[ TODO ]**</font> padding\n",
    "Make all phrases the same length. The longest phrases in the two tsv files have five tokens. Hence, we should add zeroes to all the phrases that are shorter than five. \n",
    "- we suggest using `pad_sequences`, but you can do it however you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "LF8rQwmneOlV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  45 1025  580    2    1]\n",
      " [   0    1  609   23   52]\n",
      " [   0 2541    1   18   29]\n",
      " [   1   14   81  244  113]\n",
      " [   1   18   29   66   18]]\n"
     ]
    }
   ],
   "source": [
    "# 讓phrase有相同長度\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X_train = pad_sequences(train_encoded_phrase, maxlen = 5)\n",
    "X_test = pad_sequences(test_encoded_phrase, maxlen = 5)\n",
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbE9uyk0eOlW"
   },
   "source": [
    "#### <font color=\"red\">**[ TODO ]**</font> one hot encode the labels\n",
    "- we suggest using `to_categorical`, but again, you can use whatever you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SJkFyC8_eOlX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(train['class'], 2)\n",
    "y_test = to_categorical(test['class'], 2)\n",
    "print(y_train[:5])\n",
    "print(type(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AJnIDUSeOlX"
   },
   "source": [
    "#### split training data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "r32haPqreOlY"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKthy0kTeOlY"
   },
   "source": [
    "#### <font color=\"red\">**[ TODO ]**</font> creating the embedding matrix\n",
    "The embedding matrix is used by the classification model. It should be a list of lists. Each sub-list is an embedding vector of a word and the order of all embedding vectors should be same as the word index numbering from the *tokenizer*. The tokenizer output is stored in a dictionary. You can check it using `tok.word_index.items()`.\n",
    "\n",
    "Make the embedding matrix. Our example model will need one, but you can skip it if the classification model you're using doesn't need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "id": "7zBQDNmmeOlZ",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'earn': 1, 'to': 2, 'a': 3, 'you': 4, 'and': 5, 'money': 6, 'the': 7, 'can': 8, 'more': 9, 'will': 10, 's': 11, 'that': 12, 'your': 13, 'an': 14, 'points': 15, 'income': 16, 'of': 17, 'their': 18, 'in': 19, 'for': 20}\n",
      "[ 2.41699219e-02 -1.26953125e-01 -3.59375000e-01  3.32031250e-01\n",
      "  1.17187500e-01  9.76562500e-02 -2.01416016e-02 -1.72851562e-01\n",
      " -4.54101562e-02  2.63671875e-02 -1.35742188e-01 -6.22558594e-02\n",
      "  1.23291016e-02  4.17968750e-01 -1.06201172e-02  3.80859375e-01\n",
      "  1.64794922e-02  5.93261719e-02 -5.93261719e-02  1.08398438e-01\n",
      " -2.11914062e-01 -8.49609375e-02 -1.25976562e-01  4.71191406e-02\n",
      "  6.59179688e-02 -2.31445312e-01 -7.32421875e-02 -2.45117188e-01\n",
      " -2.13623047e-02  1.69921875e-01  1.72851562e-01 -1.66015625e-01\n",
      " -1.15722656e-01 -1.17187500e-01  8.25195312e-02 -1.38671875e-01\n",
      "  1.29882812e-01  1.32812500e-01 -7.71484375e-02  1.53320312e-01\n",
      "  9.52148438e-02 -2.43164062e-01  5.37109375e-02  4.39453125e-02\n",
      " -1.65039062e-01 -4.14062500e-01  1.32812500e-01 -5.34667969e-02\n",
      "  9.71679688e-02  6.10351562e-02 -8.00781250e-02  6.78710938e-02\n",
      "  3.65234375e-01  4.88281250e-02 -2.91015625e-01  9.96093750e-02\n",
      " -3.02124023e-03 -1.28906250e-01 -1.37695312e-01 -1.83593750e-01\n",
      "  1.76757812e-01 -7.47070312e-02 -2.65625000e-01 -1.61132812e-01\n",
      " -6.44531250e-02  4.22363281e-02  6.29882812e-02 -2.50000000e-01\n",
      "  1.03027344e-01  1.19628906e-01  5.37109375e-02  1.19140625e-01\n",
      "  7.17773438e-02 -3.85742188e-02  2.94921875e-01 -4.02343750e-01\n",
      " -8.60595703e-03  1.92382812e-01  1.55029297e-02  1.30615234e-02\n",
      " -1.63085938e-01  6.98242188e-02 -2.24609375e-01  3.28125000e-01\n",
      " -2.56347656e-02 -4.19921875e-02  1.33789062e-01  6.20117188e-02\n",
      "  4.15039062e-02  1.22558594e-01 -6.22558594e-02  2.02148438e-01\n",
      " -8.10546875e-02 -9.66796875e-02  5.95703125e-02 -3.20312500e-01\n",
      "  3.65234375e-01  8.25195312e-02  8.78906250e-03 -2.19726562e-01\n",
      " -1.10839844e-01 -2.12890625e-01 -1.69921875e-01 -5.90820312e-02\n",
      "  2.44140625e-02 -1.31835938e-01 -1.90429688e-01 -2.06054688e-01\n",
      "  3.14941406e-02 -8.98437500e-02  2.40234375e-01 -6.03027344e-02\n",
      " -1.35742188e-01  2.89306641e-02  1.62109375e-01  1.64062500e-01\n",
      "  1.59179688e-01  2.18505859e-02 -1.90429688e-02 -1.68945312e-01\n",
      "  2.75390625e-01 -2.08984375e-01 -1.63574219e-02  1.13769531e-01\n",
      " -6.54296875e-02  3.28063965e-04 -8.98437500e-02 -3.88183594e-02\n",
      " -2.06054688e-01 -6.17675781e-02  1.51367188e-01 -7.47070312e-02\n",
      " -7.42187500e-02 -3.41796875e-02  8.69140625e-02 -3.00292969e-02\n",
      " -3.00781250e-01  4.14062500e-01 -1.66015625e-01  2.25585938e-01\n",
      " -1.93359375e-01 -2.55859375e-01 -1.68945312e-01 -7.91015625e-02\n",
      "  5.03540039e-03  1.09863281e-02  1.79443359e-02 -2.09960938e-01\n",
      " -6.25000000e-02 -1.61132812e-01  2.12402344e-02  1.66992188e-01\n",
      " -1.04492188e-01  4.02832031e-03 -1.05957031e-01  2.77343750e-01\n",
      "  8.20312500e-02 -1.44195557e-03  1.09375000e-01 -8.30078125e-02\n",
      "  5.39550781e-02 -3.56445312e-02 -1.60156250e-01 -1.76757812e-01\n",
      " -7.56835938e-02  7.99560547e-03  5.29785156e-02 -1.90429688e-01\n",
      "  8.54492188e-02 -2.25830078e-02 -3.10546875e-01 -2.35351562e-01\n",
      " -6.73828125e-02 -9.37500000e-02 -8.74023438e-02 -6.17675781e-02\n",
      "  2.71484375e-01  1.35742188e-01 -1.18164062e-01  1.88476562e-01\n",
      " -1.16210938e-01 -2.31445312e-01  7.66601562e-02  3.69140625e-01\n",
      " -3.16406250e-01 -1.99218750e-01  1.02050781e-01  1.17187500e-01\n",
      "  1.94091797e-02 -2.38037109e-02  9.86328125e-02 -1.33789062e-01\n",
      " -1.07421875e-01 -2.46093750e-01 -3.33984375e-01 -1.03027344e-01\n",
      "  2.09960938e-01 -1.00585938e-01 -4.33593750e-01 -1.06445312e-01\n",
      " -3.58886719e-02  2.14843750e-01  1.55273438e-01  2.61230469e-02\n",
      " -1.39770508e-02 -1.68945312e-01 -3.47656250e-01 -1.58203125e-01\n",
      " -1.70898438e-01 -6.25000000e-02  1.31835938e-01  1.46484375e-01\n",
      "  4.68750000e-02  3.08593750e-01  3.80859375e-02  3.14453125e-01\n",
      "  1.10473633e-02  1.36718750e-01 -3.32031250e-02 -2.87109375e-01\n",
      " -2.30712891e-02  5.54199219e-02  2.02148438e-01  2.29492188e-02\n",
      "  1.72851562e-01  3.20312500e-01 -1.29882812e-01 -1.60156250e-01\n",
      "  1.86523438e-01 -1.13281250e-01  3.47656250e-01 -1.12792969e-01\n",
      "  4.08203125e-01  1.85546875e-01  1.34765625e-01  6.34765625e-02\n",
      "  6.17675781e-02 -1.10351562e-01  1.39648438e-01  3.51562500e-02\n",
      "  9.91210938e-02 -1.43554688e-01  2.12402344e-02 -3.00781250e-01\n",
      " -1.66992188e-01  1.42578125e-01  7.95898438e-02 -1.19628906e-01\n",
      " -1.97265625e-01 -3.43750000e-01 -1.04980469e-01 -3.66210938e-03\n",
      " -6.29882812e-02 -2.58789062e-02 -1.57226562e-01  3.45703125e-01\n",
      " -9.39941406e-03  1.20117188e-01 -1.29882812e-01  2.02636719e-02\n",
      "  1.68945312e-01 -2.38037109e-02 -3.11279297e-02  2.20703125e-01\n",
      "  2.65625000e-01  1.12792969e-01  2.14843750e-01 -6.59179688e-02\n",
      "  2.16064453e-02 -1.37695312e-01 -6.83593750e-02 -1.44531250e-01\n",
      "  1.66015625e-01  8.54492188e-02  2.55859375e-01  1.23046875e-01\n",
      " -2.81250000e-01 -2.94921875e-01  1.18408203e-02  3.63769531e-02\n",
      "  2.09960938e-01 -2.29492188e-02 -1.05468750e-01  1.66015625e-01\n",
      " -1.78222656e-02 -1.68457031e-02 -5.81054688e-02  2.73437500e-01\n",
      " -2.53906250e-02  8.69140625e-02  1.38671875e-01  2.28515625e-01\n",
      " -2.23632812e-01 -2.42187500e-01 -2.27539062e-01 -6.25000000e-02\n",
      "  2.38281250e-01 -2.03125000e-01  2.38281250e-01 -1.53320312e-01]\n",
      "[ 0.20410156  0.01318359  0.07568359  0.28515625 -0.10888672  0.10107422\n",
      " -0.02954102  0.0480957  -0.11132812 -0.00326538 -0.09277344 -0.05761719\n",
      " -0.12988281 -0.11132812 -0.24707031  0.140625    0.07470703  0.02661133\n",
      "  0.23632812 -0.06689453  0.02856445  0.09082031  0.19140625 -0.08251953\n",
      " -0.02978516  0.11425781 -0.03442383  0.00344849 -0.00102234 -0.05444336\n",
      " -0.05615234  0.09033203 -0.03637695 -0.05761719 -0.20898438 -0.02893066\n",
      "  0.09765625  0.05200195  0.04125977  0.19238281  0.02331543 -0.24707031\n",
      "  0.34765625  0.00479126  0.10791016  0.11279297  0.08007812 -0.06689453\n",
      "  0.15136719 -0.1796875   0.00775146  0.18066406  0.0246582   0.08691406\n",
      " -0.01257324 -0.12792969 -0.00315857 -0.03088379  0.12792969  0.13476562\n",
      "  0.06298828 -0.00479126 -0.13867188 -0.05810547  0.02160645 -0.06079102\n",
      " -0.13378906  0.07763672 -0.08496094 -0.08740234  0.2734375   0.07177734\n",
      "  0.06030273 -0.22851562 -0.35742188 -0.265625    0.00294495  0.24023438\n",
      "  0.04150391  0.3203125  -0.12011719 -0.22460938  0.08154297  0.04296875\n",
      " -0.09765625 -0.13867188  0.10107422  0.27539062 -0.00379944  0.11425781\n",
      "  0.006073    0.13085938 -0.16894531 -0.07373047 -0.11035156 -0.06494141\n",
      "  0.30078125  0.09179688  0.06298828 -0.04711914 -0.13378906 -0.00090408\n",
      " -0.01806641  0.0703125  -0.3828125   0.04492188 -0.15917969  0.06030273\n",
      " -0.01519775 -0.23046875 -0.25390625 -0.12451172 -0.09814453 -0.02478027\n",
      "  0.07470703 -0.09472656 -0.09912109 -0.11767578  0.15136719 -0.09960938\n",
      " -0.00430298 -0.20214844 -0.06591797  0.14453125  0.06933594  0.06542969\n",
      " -0.10693359  0.07519531 -0.02001953  0.07128906 -0.3359375  -0.15039062\n",
      " -0.21875     0.0859375   0.0859375  -0.21484375 -0.05541992 -0.03271484\n",
      "  0.06079102  0.33789062  0.09130859 -0.10107422 -0.07568359 -0.02990723\n",
      "  0.1953125  -0.140625    0.03833008 -0.3046875   0.171875   -0.01428223\n",
      "  0.19335938  0.09667969 -0.26757812 -0.11523438 -0.04345703  0.06542969\n",
      "  0.02502441 -0.0111084  -0.17675781  0.19238281 -0.00270081  0.20019531\n",
      " -0.14941406  0.24511719  0.01397705 -0.28515625  0.171875   -0.07324219\n",
      "  0.03930664 -0.02697754 -0.29101562  0.05151367 -0.05249023 -0.14648438\n",
      " -0.13183594  0.02722168  0.12207031 -0.05371094 -0.04760742  0.01940918\n",
      " -0.05859375  0.11572266  0.05053711  0.00042725 -0.06005859 -0.0133667\n",
      " -0.13867188 -0.01574707  0.11962891  0.04833984 -0.06542969  0.20898438\n",
      " -0.14355469 -0.02514648  0.109375    0.11083984 -0.07470703 -0.02453613\n",
      " -0.07568359 -0.18457031  0.05444336  0.12890625 -0.10205078  0.10498047\n",
      " -0.02734375  0.17578125 -0.25976562 -0.20214844 -0.04467773  0.03588867\n",
      "  0.02661133  0.17480469 -0.16894531  0.03540039 -0.30273438 -0.04858398\n",
      "  0.19238281  0.06542969 -0.22070312  0.06225586 -0.07128906 -0.05761719\n",
      " -0.15039062 -0.18261719  0.11621094 -0.0625      0.18652344  0.00308228\n",
      " -0.23730469 -0.07861328 -0.05273438 -0.03344727 -0.02380371  0.11474609\n",
      "  0.03015137  0.03369141 -0.06298828  0.05810547  0.13964844 -0.04150391\n",
      "  0.01025391  0.08789062 -0.00175476 -0.17578125 -0.04858398 -0.10058594\n",
      " -0.11083984  0.27148438 -0.04272461 -0.1875      0.09912109 -0.10302734\n",
      " -0.04882812  0.05200195 -0.01470947 -0.00756836  0.06079102 -0.20019531\n",
      " -0.05371094 -0.01391602 -0.03686523 -0.20507812 -0.18554688 -0.11376953\n",
      "  0.02453613  0.13671875  0.03112793 -0.00534058 -0.17480469 -0.02258301\n",
      " -0.10791016  0.34375     0.16113281  0.30273438 -0.09082031 -0.02685547\n",
      "  0.00588989 -0.19335938 -0.14257812  0.05004883  0.01733398 -0.19140625\n",
      " -0.00897217  0.08740234 -0.06787109  0.13476562 -0.06103516 -0.203125\n",
      "  0.00958252  0.16601562 -0.01635742  0.1328125  -0.23828125 -0.10595703\n",
      "  0.08691406 -0.10058594  0.04956055 -0.21191406 -0.1328125   0.10839844]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "hehe = tok.word_index.items()\n",
    "\n",
    "print(dict(list(hehe)[:20]))\n",
    "# tok.word_index.items() 表示單字庫內 個別單字的id num\n",
    "# {'earn': 1, 'to': 2, 'a': 3, 'you': 4, 'and': 5, 'money': 6, 'the': 7, 'can': 8 ...}\n",
    "# tok 跟著[train, test]\n",
    "#w2v_model\n",
    "\n",
    "import numpy as np\n",
    "embedding_matrix = np.zeros((len(tok.word_index) + 1, 300))\n",
    "for word, i in tok.word_index.items():\n",
    "    # i start from 1\n",
    "    # train, test set 中的單字如果在model中找得到 就填入matrix\n",
    "    if word in w2v_model.vocab:\n",
    "        embedding_matrix[i-1] = w2v_model[word]\n",
    "\n",
    "#print(w2v_model['earn'])\n",
    "print(embedding_matrix[0])\n",
    "print(embedding_matrix[3])\n",
    "print(embedding_matrix[4])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c9JvMZaeOlZ"
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpgeqgmWeOlZ"
   },
   "source": [
    "#### build model\n",
    "<font color=\"red\">**[ TODO ]**</font> Please build your classification model by ***keras*** here. Don't worry if you don't know how, just use the one given below. Feel free to make any changes or even build your own.\n",
    "\n",
    "You **must** use the pre-trained word2vec model to represent the words of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "XRlAGm9teOla"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Flatten , Embedding, LSTM, LSTM, ReLU, Dropout\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,output_dim=300,input_length=5,embeddings_initializer=Constant(embedding_matrix)))\n",
    "model.add(LSTM(64,return_sequences=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2,activation='sigmoid')) \n",
    "model.compile(optimizer=RMSprop(lr=1e-3),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "7EVyMofjeOla"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 5, 300)            1739400   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 1,832,970\n",
      "Trainable params: 1,832,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSG0J5P7eOla"
   },
   "source": [
    "#### train\n",
    "Train classification model here.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Adjust the hyperparameter to optimize the validation accuracy and validation loss.\n",
    "\n",
    "* The higher the accuracy, the better; the lower the validation, the better.\n",
    "* **number of epoch** and **batch size** are the most important\n",
    "  * Start with a smaller number of epochs first--it is directly correlated to the training time, and you don't want to spend too much time waiting!\n",
    "  * Usually the larger the batch size the better, but the batch size you are able to use depends on you computing power, so start small and increase gradually. It is recommended to use powers of 2 (2, 4, 8, 16, ...) for batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "MSs4f9ELeOlb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "30/30 [==============================] - 4s 27ms/step - loss: 0.5155 - accuracy: 0.7556 - val_loss: 0.3186 - val_accuracy: 0.8407\n",
      "Epoch 2/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2902 - accuracy: 0.8532 - val_loss: 0.2257 - val_accuracy: 0.9177\n",
      "Epoch 3/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1892 - accuracy: 0.9238 - val_loss: 0.1651 - val_accuracy: 0.9477\n",
      "Epoch 4/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1230 - accuracy: 0.9601 - val_loss: 0.1106 - val_accuracy: 0.9649\n",
      "Epoch 5/30\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.0828 - accuracy: 0.9741 - val_loss: 0.0798 - val_accuracy: 0.9742\n",
      "Epoch 6/30\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.0647 - accuracy: 0.9785 - val_loss: 0.0984 - val_accuracy: 0.9559\n",
      "Epoch 7/30\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.0618 - accuracy: 0.9780 - val_loss: 0.0621 - val_accuracy: 0.9768\n",
      "Epoch 8/30\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.0514 - accuracy: 0.9815 - val_loss: 0.0571 - val_accuracy: 0.9812\n",
      "Epoch 9/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0461 - accuracy: 0.9837 - val_loss: 0.0574 - val_accuracy: 0.9812\n",
      "Epoch 10/30\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.0432 - accuracy: 0.9851 - val_loss: 0.0549 - val_accuracy: 0.9805\n",
      "Epoch 11/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0426 - accuracy: 0.9844 - val_loss: 0.0588 - val_accuracy: 0.9799\n",
      "Epoch 12/30\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.0409 - accuracy: 0.9842 - val_loss: 0.0528 - val_accuracy: 0.9828\n",
      "Epoch 13/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0379 - accuracy: 0.9861 - val_loss: 0.0579 - val_accuracy: 0.9798\n",
      "Epoch 14/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0361 - accuracy: 0.9862 - val_loss: 0.0529 - val_accuracy: 0.9828\n",
      "Epoch 15/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0343 - accuracy: 0.9866 - val_loss: 0.0526 - val_accuracy: 0.9831\n",
      "Epoch 16/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0323 - accuracy: 0.9870 - val_loss: 0.0596 - val_accuracy: 0.9794\n",
      "Epoch 17/30\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.0325 - accuracy: 0.9874 - val_loss: 0.0571 - val_accuracy: 0.9803\n",
      "Epoch 18/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0300 - accuracy: 0.9883 - val_loss: 0.0523 - val_accuracy: 0.9829\n",
      "Epoch 19/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0289 - accuracy: 0.9879 - val_loss: 0.0549 - val_accuracy: 0.9820\n",
      "Epoch 20/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0308 - accuracy: 0.9870 - val_loss: 0.0542 - val_accuracy: 0.9828\n",
      "Epoch 21/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0279 - accuracy: 0.9891 - val_loss: 0.0657 - val_accuracy: 0.9765\n",
      "Epoch 22/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0275 - accuracy: 0.9876 - val_loss: 0.0600 - val_accuracy: 0.9829\n",
      "Epoch 23/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0287 - accuracy: 0.9883 - val_loss: 0.0624 - val_accuracy: 0.9798\n",
      "Epoch 24/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0276 - accuracy: 0.9889 - val_loss: 0.0622 - val_accuracy: 0.9776\n",
      "Epoch 25/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0277 - accuracy: 0.9882 - val_loss: 0.0590 - val_accuracy: 0.9814\n",
      "Epoch 26/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0232 - accuracy: 0.9907 - val_loss: 0.0598 - val_accuracy: 0.9810\n",
      "Epoch 27/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0256 - accuracy: 0.9891 - val_loss: 0.0623 - val_accuracy: 0.9806\n",
      "Epoch 28/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0264 - accuracy: 0.9885 - val_loss: 0.0589 - val_accuracy: 0.9827\n",
      "Epoch 29/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0262 - accuracy: 0.9888 - val_loss: 0.0678 - val_accuracy: 0.9756\n",
      "Epoch 30/30\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0254 - accuracy: 0.9885 - val_loss: 0.0630 - val_accuracy: 0.9784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4e001e1198>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,validation_data=(X_val,y_val), epochs=30 , batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJIEsewSeOlc"
   },
   "source": [
    "#### test\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Test your model by test.tsv and output the accuracy. Beat the accuracy baseline: **0.98** for extra points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nDzmeBV4eOlc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1104 - accuracy: 0.9575\n",
      "0.9574999809265137\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(X_test,y_test)\n",
    "print(accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxVkZUuFeOlc"
   },
   "source": [
    "## Show wrong prediction results\n",
    "Observing wrong prediction result may help you improve your prediction.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> show the wrong prediction results like this: \n",
    "\n",
    "<img src=\"https://imgur.com/BOTMyZH.jpg\" width=30%><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch(length):\n",
    "    if length > 30 :\n",
    "        return \"\\t\"\n",
    "    elif length >= 23 and length <= 30:\n",
    "        return \"\\t\\t\"\n",
    "    elif length >= 20 and length < 23:\n",
    "        return \"\\t\\t\\t\"\n",
    "    elif length >= 15 and length  < 20:\n",
    "        return \"\\t\\t\\t\"\n",
    "    else:\n",
    "        return \"\\t\\t\\t\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "lVqH4lvleOld"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.float32'>\n",
      "0.9999553\n",
      "3.0325447e-05\n",
      "ngram\t\t\t\t\tlabel\t\tpredict\n",
      "earn barely enough \t\t\t 0 \t\t 1\n",
      "earn a 4.0 grade point \t\t\t 0 \t\t 1\n",
      "earn your spot \t\t\t\t 0 \t\t 1\n",
      "earn an Air Force commission \t\t 0 \t\t 1\n",
      "earn your Masters Degree \t\t 1 \t\t 0\n",
      "earn LEED credit \t\t\t 0 \t\t 1\n",
      "earn instead of sharing it \t\t 0 \t\t 1\n",
      "earn the final spot \t\t\t 0 \t\t 1\n",
      "earn free book credit \t\t\t 0 \t\t 1\n",
      "that which ye earn \t\t\t 0 \t\t 1\n",
      "earn the victory \t\t\t 0 \t\t 1\n",
      "earn a great salary \t\t\t 0 \t\t 1\n",
      "earn a poor credit \t\t\t 0 \t\t 1\n",
      "earn your confidence and respect \t 0 \t\t 1\n",
      "earn more commission \t\t\t 0 \t\t 1\n",
      "earn a > living \t\t\t 0 \t\t 1\n",
      "earn an extra money \t\t\t 0 \t\t 1\n",
      "earn money from earn money \t\t 0 \t\t 1\n",
      "earn your confidence so that \t\t 0 \t\t 1\n",
      "them earn a better living \t\t 1 \t\t 0\n",
      "power projects nationwide earn \t\t 0 \t\t 1\n",
      "earn a referral fee \t\t\t 0 \t\t 1\n",
      "Most do not earn \t\t\t 0 \t\t 1\n",
      "earn the nomination \t\t\t 0 \t\t 1\n",
      "earn your commission ! \t\t\t 1 \t\t 0\n",
      "money , unless you earn \t\t 0 \t\t 1\n",
      "earn money can be found \t\t 1 \t\t 0\n",
      "earn a ' living \t\t\t 0 \t\t 1\n",
      "earn your art degree \t\t\t 0 \t\t 1\n",
      "earn victory \t\t\t\t 0 \t\t 1\n",
      "earn mega money \t\t\t 0 \t\t 1\n",
      "earn a living \t\t\t\t 0 \t\t 1\n",
      "earn an Emmy nomination \t\t 0 \t\t 1\n",
      "- earn affiliate income \t\t 1 \t\t 0\n",
      "more you earn . \" \t\t\t 1 \t\t 0\n",
      "earn a Grammy nomination \t\t 0 \t\t 1\n",
      "earn commission \t\t\t 0 \t\t 1\n",
      "earn some extra credit \t\t\t 0 \t\t 1\n",
      "earn \" their money \t\t\t 0 \t\t 1\n",
      "earn 16 credit \t\t\t\t 0 \t\t 1\n",
      "earn PDH credit \t\t\t 0 \t\t 1\n",
      "earn fantastic commission \t\t 0 \t\t 1\n",
      "earn some credit \t\t\t 0 \t\t 1\n",
      "earn ( income \t\t\t\t 0 \t\t 1\n",
      "earn dual credit \t\t\t 0 \t\t 1\n",
      "earn a standard diploma \t\t 0 \t\t 1\n",
      "what they can earn , \t\t\t 1 \t\t 0\n",
      "earn residual income \t\t\t 0 \t\t 1\n",
      ", earn on average \t\t\t 1 \t\t 0\n",
      "Family Meeting can earn \t\t 0 \t\t 1\n",
      ", earn or get money \t\t\t 1 \t\t 0\n",
      "earn money | earn money \t\t 0 \t\t 1\n",
      "what I earn , \t\t\t\t 1 \t\t 0\n",
      "earn immediately money \t\t\t 0 \t\t 1\n",
      "earn money online earn money \t\t 0 \t\t 1\n",
      "earn cash make money \t\t\t 0 \t\t 1\n",
      "earn spot \t\t\t\t 0 \t\t 1\n",
      "earn an independent income \t\t 0 \t\t 1\n",
      "earn ½ credit \t\t\t\t 0 \t\t 1\n",
      "earn extra money \t\t\t 0 \t\t 1\n",
      "earn an income \t\t\t\t 0 \t\t 1\n",
      "earn the confidence and respect \t 0 \t\t 1\n",
      "earn 75 % interest \t\t\t 0 \t\t 1\n",
      "earn , earned income \t\t\t 0 \t\t 1\n",
      "earn money from Our Beer \t\t 1 \t\t 0\n",
      "earn his victory \t\t\t 0 \t\t 1\n",
      "him to earn \t\t\t\t 0 \t\t 1\n",
      "earn Free Gear with Bonus \t\t 1 \t\t 0\n",
      "earn the last spot \t\t\t 0 \t\t 1\n",
      "earn + money \t\t\t\t 0 \t\t 1\n",
      "earn savings income \t\t\t 0 \t\t 1\n",
      "it these girls earn \t\t\t 0 \t\t 1\n",
      "earn some extra income \t\t\t 0 \t\t 1\n",
      "earn money , earn money \t\t 0 \t\t 1\n",
      "earn their credit \t\t\t 0 \t\t 1\n",
      "earn to qualify for what \t\t 0 \t\t 1\n",
      "earn profit \t\t\t\t 0 \t\t 1\n",
      "earn + a + degree \t\t\t 0 \t\t 1\n",
      "earn a profit \t\t\t\t 0 \t\t 1\n",
      "earn a credit \t\t\t\t 0 \t\t 1\n",
      "earn the Most \t\t\t\t 0 \t\t 1\n",
      "- money earn - \t\t\t\t 1 \t\t 0\n",
      "earn virtual money \t\t\t 0 \t\t 1\n",
      "earn additional income \t\t\t 0 \t\t 1\n",
      "earn fake money \t\t\t 0 \t\t 1\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test[:])\n",
    "print(type(predictions))\n",
    "print(type(predictions[0]))\n",
    "print(type(predictions[0][0]))\n",
    "print(predictions[0][0])\n",
    "print(predictions[0][1])\n",
    "print(f'ngram\\t\\t\\t\\t\\tlabel\\t\\tpredict')\n",
    "for x, i in zip(predictions, range(len(X_test))):\n",
    "    space = switch(len(test[\"phrase\"][i]))\n",
    "    if x[0] > x[1] and y_test[i][0] != 1.0: # [1 0] -> 0\n",
    "        print(f'{test[\"phrase\"][i]} {space} 0 \\t\\t 1')\n",
    "        #print(x)\n",
    "        #print(y_test[i])\n",
    "    if x[0] < x[1] and y_test[i][1] != 1.0:  # [0 1] -> 0\n",
    "        print(f'{test[\"phrase\"][i]} {space} 1 \\t\\t 0')\n",
    "        #print(test['phrase'][i])\n",
    "        #print(x)\n",
    "        #print(f'{y_test[i]} \\n')\n",
    "#print(predictions)\n",
    "#print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('earn a > living')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KprbNm7KeOle"
   },
   "source": [
    "## TA's Notes\n",
    "\n",
    "If you complete the Assignment, please use [this link](https://docs.google.com/spreadsheets/d/1OKbXhcv6E3FEQDPnbHEHEeHvpxv01jxugMP7WwnKqKw/edit#gid=258852025) to reserve demo time.  \n",
    "The score is only given after TAs review your implementation, so <u>**make sure you make a appointment with a TA before you miss the deadline**</u> .  <br>After demo, please upload your assignment to elearn. You just need to hand in this ipynb file and rename it as XXXXXXXXX(Your student ID).ipynb.\n",
    "<br>Note that **late submission will not be allowed**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQl6lTDzeOlf"
   },
   "source": [
    "## Learning Resource\n",
    "[Deep Learning with Python](https://tanthiamhuat.files.wordpress.com/2018/03/deeplearningwithpython.pdf)\n",
    "\n",
    "[Classification on IMDB](https://keras.io/examples/nlp/bidirectional_lstm_imdb/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
